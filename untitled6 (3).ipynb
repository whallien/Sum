{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2NhUs3tOZ0N"
      },
      "outputs": [],
      "source": [
        "#pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrFTfZMQO9Le"
      },
      "outputs": [],
      "source": [
        "#pip install huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKNCVIQmYiHg",
        "outputId": "d7be2b62-12f8-489e-f1e0-7f6b27a5c81d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QQp2W8bQc3C",
        "outputId": "6e4382af-1941-428c-f682-c97978accc04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVOK_RjeV7YG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from rarfile import RarFile\n",
        "\n",
        "# Распаковка данных из архивов\n",
        "with ZipFile('archive.zip', 'r') as z:\n",
        "    z.extractall()\n",
        "with RarFile('Info.rar', 'r') as r:\n",
        "    r.extractall()\n",
        "\n",
        "# Считываем информацию из csv файлов\n",
        "info_files = os.listdir('Info')\n",
        "info_data = []\n",
        "for file in info_files:\n",
        "    data = pd.read_csv('Info/' + file, delimiter=',', on_bad_lines='skip', engine='python')\n",
        "    info_data.append(data)\n",
        "\n",
        "info_df = pd.concat(info_data)\n",
        "\n",
        "# Считываем тексты статей из txt файлов\n",
        "data_3 = os.listdir('data_3')\n",
        "data_3_1 = os.listdir('data_3_1')\n",
        "\n",
        "data_3_texts = []\n",
        "data_3_1_texts = []\n",
        "\n",
        "# Путь к папке data_3\n",
        "data_3_path = '/content/data_3'\n",
        "folders = os.listdir(data_3_path)\n",
        "\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(data_3_path, folder)\n",
        "    # Проверяем, является ли текущий элемент папкой\n",
        "    if os.path.isdir(folder_path):\n",
        "        files = os.listdir(folder_path)\n",
        "        # Перебираем файлы в текущей папке\n",
        "        for file in files:\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            # Проверяем, является ли текущий элемент файлом\n",
        "            if os.path.isfile(file_path) and file.endswith('.txt'):\n",
        "                 with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    data_3_texts.append(f.read())\n",
        "\n",
        "data_3_1_path = '/content/data_3'\n",
        "folders = os.listdir(data_3_1_path)\n",
        "\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(data_3_1_path, folder)\n",
        "    # Проверяем, является ли текущий элемент папкой\n",
        "    if os.path.isdir(folder_path):\n",
        "        files = os.listdir(folder_path)\n",
        "        # Перебираем файлы в текущей папке\n",
        "        for file in files:\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            # Проверяем, является ли текущий элемент файлом\n",
        "            if os.path.isfile(file_path) and file.endswith('.txt'):\n",
        "                 with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    data_3_1_texts.append(f.read())\n",
        "# Объединяем информацию из csv файлов с текстами статей\n",
        "info_df_reset = info_df.reset_index(drop=True)\n",
        "data_3_df = pd.DataFrame(data_3_texts, columns=['text'])\n",
        "data_3_df['name'] = info_df_reset['Field1'][:len(data_3_texts)]\n",
        "\n",
        "data_3_1_df = pd.DataFrame(data_3_1_texts, columns=['text'])\n",
        "data_3_1_df['name'] = info_df_reset['Field1'][:len(data_3_1_texts)]\n",
        "\n",
        "# Сохраняем полученные данные\n",
        "data_3_df.to_csv('data_3.csv', index=False, escapechar='\\\\')\n",
        "data_3_1_df.to_csv('data_3_1.csv', index=False, escapechar='\\\\')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kjTmaTAJewmQ"
      },
      "outputs": [],
      "source": [
        "#info_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PaF0ahr177J_"
      },
      "outputs": [],
      "source": [
        "#!pip install spacy\n",
        "#!python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2-rcqzs686CF"
      },
      "outputs": [],
      "source": [
        "#!pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g36lobEzLXsr"
      },
      "outputs": [],
      "source": [
        "#pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYzOy9oOhd5j"
      },
      "outputs": [],
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Загрузка модели и токенизатора mBART\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "# Установка языка модели\n",
        "tokenizer.src_lang = \"ru_RU\"\n",
        "tokenizer.tgt_lang = \"ru_RU\"\n",
        "\n",
        "# Функция для суммаризации текста\n",
        "def summarize_text(texts, max_length=1024, batch_size=8):\n",
        "    summaries = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Summarizing\"):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=True)\n",
        "        inputs = {k: v for k, v in inputs.items()}\n",
        "        summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summaries.extend(tokenizer.batch_decode(summary_ids, skip_special_tokens=True))\n",
        "    return summaries\n",
        "\n",
        "# Загрузка данных\n",
        "# Сэмплирование\n",
        "sampled_data_3_df = data_3_df.sample(n=100, random_state=42)\n",
        "\n",
        "# Создание аннотаций с использованием прогресс-бара\n",
        "texts = sampled_data_3_df['text'].tolist()\n",
        "sampled_data_3_df['generated_summary'] = summarize_text(texts)\n",
        "\n",
        "# Сохранение данных с аннотациями\n",
        "sampled_data_3_df.to_csv('sampled_data_3_with_summaries_and_generated.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFXGvlVwCX4J"
      },
      "outputs": [],
      "source": [
        "sampled_data_3_df[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLbjvngqEsNX"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Проверка наличия GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Инициализация модели BART (меньшая модель)\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base').to(device)\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "\n",
        "# Определение класса Dataset с разбивкой длинных текстов\n",
        "class SummaryDataset(Dataset):\n",
        "    def __init__(self, texts, summaries, tokenizer, max_length=512, summary_max_length=128):\n",
        "        self.encodings = []\n",
        "        self.labels = []\n",
        "        for text, summary in zip(texts, summaries):\n",
        "            tokens = tokenizer(text, truncation=False, padding=False)['input_ids']\n",
        "            label_tokens = tokenizer(summary, truncation=True, padding=False, max_length=summary_max_length)['input_ids']\n",
        "            for i in range(0, len(tokens), max_length):\n",
        "                self.encodings.append(tokens[i:i+max_length])\n",
        "                self.labels.append(label_tokens[:summary_max_length])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {'input_ids': torch.tensor(self.encodings[idx]), 'labels': torch.tensor(self.labels[idx])}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings)\n",
        "\n",
        "# Загрузка данных для обучения\n",
        "sampled_data_3_df = pd.read_csv('sampled_data_3_with_summaries_and_generated.csv')\n",
        "\n",
        "# Проверка количества доступных данных\n",
        "n_samples = min(100, sampled_data_3_df.shape[0])  # Берем максимум 100 образцов или меньше, если данных меньше\n",
        "\n",
        "# Сэмплирование для тестирования\n",
        "sampled_data_3_df = sampled_data_3_df.sample(n=n_samples, random_state=42)\n",
        "\n",
        "# Разделение данных на обучающую и валидационную выборки\n",
        "train_texts, val_texts, train_summaries, val_summaries = train_test_split(\n",
        "    sampled_data_3_df['text'], sampled_data_3_df['generated_summary'], test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Создание датасетов с разбивкой длинных текстов\n",
        "train_dataset = SummaryDataset(train_texts.tolist(), train_summaries.tolist(), tokenizer)\n",
        "val_dataset = SummaryDataset(val_texts.tolist(), val_summaries.tolist(), tokenizer)\n",
        "\n",
        "# Уменьшение размера батча для уменьшения использования памяти\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "# Очистка неиспользуемой памяти\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Определение функции обучения с градиентным накоплением\n",
        "def train(model, train_loader, val_loader, epochs=3, lr=5e-5, gradient_accumulation_steps=8):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "            total_loss += loss.item() * gradient_accumulation_steps\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Валидация\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                total_val_loss += loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}\")\n",
        "        model.train()\n",
        "\n",
        "# Обучение модели\n",
        "train(model, train_loader, val_loader, epochs=3, lr=5e-5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}